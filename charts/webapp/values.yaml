# Default values for webapp.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# --name is the GitHub repository name of this application deployment
name: ""

# --nameOverride allows partial override of the name
nameOverride: {}

# --fullnameOverride allows full override of the name
fullnameOverride: {}

global:
  # --cluster sets the Cluster Name
  cluster:
    domain: cluster.local
    name: ""
  imageRegistry: ""
  # --env sets the Environment Name (dev, mng, prd)
  env: ""
  # --prometheus sets the Prometheus server URL
  prometheus:
    # --server sets prometheus endpoint
    server: ""
  # -- Network configuration
  network:
    # --domain sets the Default Domain
    domain: ""
  # --otel sets the endpoint for OpenTelemetry collector
  otel:
    endpoint: ""
    port: ""
    argument: "0.25"

# --commonLabels sets common labels for all resources
commonLabels: {}
# team_name: dev
# --restartPolicy is the object to specify the restart policy for the container
restartPolicy: Always
# --terminationGracePeriodSeconds is the number of seconds to wait before terminating a pod
terminationGracePeriodSeconds: 30
# --command is the object to specify the command to run in the container
command: []
# --args is the object to specify the arguments to run in the container
args: []

# --namespace configuration
namespace:
  # --Specifies whether the namespace is enabled
  enabled: false
  # --Labels to be added to the namespace
  labels: {}
  # --Annotations to be added to the namespace
  annotations: {}
  # --Example annotation to enable Jaeger tracing sidecar injection
  # sidecar.jaegertracing.io/inject: "true"

# --autoReloader enable auto reloader to restart the container when the configMap is updated
autoReloader: true

# --configMaps is the object to configure an array of configMaps
configMaps:
  # --enabled is the flag to sinalize this funcionality is enabled
  enabled: false
  # --data is the object to configure the data of the configMap
  data: {}

# --consumers is the object to configure an array of consumers
consumers:
  # --annotations to be added to the consumers
  annotations: {}
  # --terminationGracePeriodSeconds configures terminationGracePeriodSeconds
  terminationGracePeriodSeconds: 30
  # --list is the array of consumer definition
  list:
    []
    # - name: required
    #   command:
    #     - required

# cronjobs list of cronjobs to be included in the application
cronjobs:
  # --suspend used to disable all cronjobs in the list
  suspend: false
  # --list is an array of spec for create multiples cronjobs
  list:
    []
    # - name: cron1
    #   schecule: "* * * * *"
    #   command:
    #     - echo "ok"

# --deployment Disabled Deployment
deployment:
  # --enabled is the flag to sinalize this funcionality is enabled
  enabled: true
  # --strategyType is the type of deployment strategy to use
  strategyType: RollingUpdate
  # --annotations to be added to the deployment
  annotations: {}
  # --labels to be added to the deployment
  labels: {}

# --argoRollouts enable Argo Rollouts Deployment
argoRollouts:
  # --Specifies whether Argo Rollouts is enabled
  enabled: false
  # --Specifies the number of old ReplicaSets to retain for rollback purposes
  revisionHistoryLimit: 3
  # --Specifies whether analysis runs should be created during the rollout
  analyses:
    # --enabled specifies whether analysis runs should be created during the rollout
    enabled: true
    # --Specifies the success condition for the analysis, as a percentage
    successCondition: "len(result) == 0 || isNaN(result[0]) || isInf(result[0]) || result[0] >= 0.95"
    # --Specifies the maximum number of failed analysis runs allowed before the rollout fails
    failureLimit: 3
    # Name of the metric to be used in the analysis template
    metricName: "success-rate"
    # The initial delay before the first measurement is taken
    initialDelay: "30s"
    # The interval between each measurement
    interval: "20s"

  # --Specifies whether the stable ReplicaSet should be dynamically scaled during rollout
  dynamicStableScale: true

  # --strategy is the object to configure the rollout strategy
  strategy:
    steps:
      # --Sets the percentage of traffic to send to the new version
      - setWeight: 5
      # --Pauses the rollout for a specified duration
      - pause:
          duration: 10s
      # --Sets the percentage of traffic to send to the new version
      - setWeight: 20
      # --Pauses the rollout for a specified duration
      - pause:
          duration: 10s
      # --Sets the percentage of traffic to send to the new version
      - setWeight: 40
      # --Pauses the rollout for a specified duration
      - pause:
          duration: 10s
      # --Sets the percentage of traffic to send to the new version
      - setWeight: 60
      # --Pauses the rollout for a specified duration
      - pause:
          duration: 10s
      # --Sets the percentage of traffic to send to the new version
      - setWeight: 80
      # --Pauses the rollout for a specified duration
      - pause:
          duration: 10s

# --service
service:
  # --service.enabled to enable and disable the creation of service
  enabled: true
  # --Service An abstract way to expose an application running on a set of Pods as a network service.
  type: ClusterIP
  # --service.port Define the Service Port
  port:
    # --Name of the port, used for service discovery
    name: tcp-node
    # --Port is the port your application runs under
    port: 80
    # --targetPort is the port your application runs under
    targetPort: 8080
  # --Annotations to add to the service
  annotations: {}
  # --Labels to add to the service
  labels: {}
  # --service.externalDns to enable and disable the creation of external DNS
  externalDns:
    # --Enable or disable external DNS for the service
    enabled: false
  # --The port to expose on each node in a cluster, only used if type is NodePort or LoadBalancer
  nodePort: {}

# --istio Set default Istio
istio:
  # enable istio-injection label on namespace
  enabled: true
  # --istio.virtualServices Set Istio virtualServices parameters
  virtualServices:
    # --istio.virtualServices.enable Set Istio virtualServices enabled
    enabled: true
    public: false
    custom:
      hosts: []

  # --PeerAuthentication defines how traffic will be tunneled (or not) to the sidecar.
  peerAuthentication:
    # --enable peerAuthentication
    enabled: true
    # --set peerAuthentication mode, values (UNSET, DISABLE, PERMISSIVE, STRICT)
    mode: PERMISSIVE

  # --gateways set default gateway for virtual-service
  gateways: istio-ingress/istio-ingressgateway

  # --authorizationPolicy set default authorization policy
  authorizationPolicy:
    # --enable authorizationPolicy
    enabled: true
    # Optional, since default is ALLOW
    action: ALLOW
    # No rules means "ALLOW ALL" in Istio when action is ALLOW
    rules:
      - from:
          - source:
              namespaces: ["*"]

# --ServiceAccount A service account provides an identity for processes that run in a Pod, about more: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  name: {}
  # --Specifies whether a service account should be created
  enabled: true
  # --Annotations to add to the service account
  annotations: {}
  # --serviceAccount.annotations Automont Service Account Token
  automountServiceAccountToken: true

# --replicaCount is used when autoscaling.enabled is false to set a manually number of pods
replicaCount: 1

# --autoscaling is the main object of autoscaling
autoscaling:
  # --enabled is the flag to sinalize this funcionality is enabled
  enabled: true
  # --minReplicas is the number of mim pods to be running
  minReplicas: 1
  # --maxReplicas is the number of maximum scaling pods
  maxReplicas: 2
  # --targetCPUUtilizationPercentage is the percentage of cpu when reached to scale new pods
  targetCPUUtilizationPercentage: 80
  # --targetMemoryUtilizationPercentage is the percentage of memoty when reached to scale new pods
  targetMemoryUtilizationPercentage: 80
  # --customRules is a place to customize your application autoscaler using the original API available at: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  customRules:
    []
    # - type: Resource
    #   resource:
    #     name: memory
    #     target:
    #       type: Utilization
    #       averageUtilization: 20

# --container is the object to configure the container
container:
  # --port is the port your application runs under
  port: 8080
  # --readOnlyRootFilesystem is the prop to setup the container root filesystem as read-only
  readOnlyRootFilesystem: true

# --image is the object to specify the image to run in the deployment
image:
  # --repository: is the registry of your application ex:556684128444.dkr.ecr.us-east-1.amazonaws.com/YOU-APP-ECR-REPO-NAME
  # if empty this helm will auto generate the image using aws.registry/values.name:values.image.tag
  repository: ""
  # --pullPolicy is the prop to setup the behavior of pull police. options is: IfNotPresent \| allways
  pullPolicy: IfNotPresent
  # --especify the tag of your image to deploy
  tag: "latest"

# --imagePullSecrets secret used to download image on private container registry
imagePullSecrets:
  []
  # --imagePullSecrets.enabled create secret do pull docker images in private registrys
  # enabled: true
  # name: ghcr-secret

# --resources set deployment resources
resources:
  {}
  # --Resource requests are the minimum amount of CPU and memory that Kubernetes guarantees for the container
  # requests:
  #   cpu: {}
  #   memory: {}
  # # --Resource limits are the maximum amount of CPU and memory that the container is allowed to use
  # limits:
  #   cpu:
  #   memory:

## Health Check
# --livenessProbe indicates whether the application is running and alive
livenessProbe:
  # --Specifies whether the liveness probe is enabled
  enabled: false
  # --The HTTP path to check for liveness
  path: /health-check/liveness
  # --The scheme to use for the liveness probe (e.g., HTTP or HTTPS)
  scheme: HTTP
  # --Specifies a command to run inside the container to determine liveness
  exec: {}
  # --Number of seconds after the container has started before liveness probes are initiated
  initialDelaySeconds: 10
  # --Number of seconds after which the liveness probe times out
  timeoutSeconds: 3
  # --How often (in seconds) to perform the liveness probe
  periodSeconds: 10
  # --Minimum consecutive failures for the probe to be considered failed after having succeeded
  failureThreshold: 3
  # --Minimum consecutive successes for the probe to be considered successful after having failed
  successThreshold: 1
  # --httpHeaders specifies custom headers to set in the request
  httpHeaders: []

# --readinessProbe indicates whether the application is ready to serve requests
readinessProbe:
  # --Specifies whether the readiness probe is enabled
  enabled: false
  # --The HTTP path to check for readiness
  path: /health-check/readiness
  # --The scheme to use for the readiness probe (e.g., HTTP or HTTPS)
  scheme: HTTP
  # --Specifies a command to run inside the container to determine readiness
  exec: {}
  # --Number of seconds after the container has started before readiness probes are initiated
  initialDelaySeconds: 10
  # --Number of seconds after which the readiness probe times out
  timeoutSeconds: 3
  # --How often (in seconds) to perform the readiness probe
  periodSeconds: 10
  # --Minimum consecutive failures for the probe to be considered failed after having succeeded
  failureThreshold: 3
  # --Minimum consecutive successes for the probe to be considered successful after having failed
  successThreshold: 1
  # --httpHeaders specifies custom headers to set in the request
  httpHeaders: []

# Actuator settings for exposing application metrics and health checks
actuator:
  # -- If enabled, create default actuator path and metrics
  enabled: false
  port:
    # -- The name of the port for actuator metrics
    name: tcp-metrics
    # -- The port number for actuator metrics
    port: 9090
    # -- The target port in the container for actuator metrics
    targetPort: 9090
    # -- The protocol used to service
    protocol: TCP
  metrics:
    # -- The path to expose Prometheus metrics
    path: /actuator/prometheus
  liveness:
    # -- The path to check liveness of the application
    path: /actuator/health/liveness
    # --The scheme to use for the readiness probe (e.g., HTTP or HTTPS)
    scheme: HTTP
    # --Number of seconds after the container has started before readiness probes are initiated
    initialDelaySeconds: 120
    # --Number of seconds after which the readiness probe times out
    timeoutSeconds: 3
    # --How often (in seconds) to perform the readiness probe
    periodSeconds: 10
    # --Minimum consecutive failures for the probe to be considered failed after having succeeded
    failureThreshold: 3
    # --Minimum consecutive successes for the probe to be considered successful after having failed
    successThreshold: 1
    # --httpHeaders specifies custom headers to set in the request
    httpHeaders: []
  readiness:
    # -- The path to check readiness of the application
    path: /actuator/health/readiness
    # --The scheme to use for the readiness probe (e.g., HTTP or HTTPS)
    scheme: HTTP
    # --Number of seconds after the container has started before readiness probes are initiated
    initialDelaySeconds: 120
    # --Number of seconds after which the readiness probe times out
    timeoutSeconds: 3
    # --How often (in seconds) to perform the readiness probe
    periodSeconds: 10
    # --Minimum consecutive failures for the probe to be considered failed after having succeeded
    failureThreshold: 3
    # --Minimum consecutive successes for the probe to be considered successful after having failed
    successThreshold: 1
    # --httpHeaders specifies custom headers to set in the request
    httpHeaders: []

# --instrumentation set default auto-instrumentation, allowed values dotnet, go, java, nodejs and python
instrumentation:
  # --enabled specifies whether instrumentation is enabled
  enabled: false
  # --language specifies the programming language of the application for instrumentation
  # --allowed values: dotnet, go, java, nodejs and python
  language: ""

# --nodeSelector allows you to constrain a Pod to only be able to run on particular node(s)
nodeSelector: {}

# --affinity allows you to define rules for pod scheduling based on node labels
affinity: {}

# --tolerations allows the pods to schedule onto nodes with taints
tolerations: []

# --topologySpreadConstraints allows you to constrain the pods to run on nodes with a certain topology
topologySpreadConstraints:
  # --topologySpreadConstraints.enabled specifies whether topology spread constraints should be applied
  enabled: true
  # --topologySpreadConstraints.maxSkew specifies the maximum skew allowed among the topologies
  maxSkew: 1
  # --topologySpreadConstraints.topologyKey specifies the key of the topology
  topologyKey: "topology.kubernetes.io/zone"
  # --topologySpreadConstraints.whenUnsatisfiable specifies the action to take when a constraint is not satisfied
  whenUnsatisfiable: "ScheduleAnyway"

# --podAnnotations adds custom annotations to the pod
podAnnotations: {}

# --podSecurityContext sets the security context for the pod
podSecurityContext:
  {}
  # fsGroup: 2000

# --securityContext sets the security context for the container
securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

envs:
  []
  # examples:
  # please note that when the value is a number, you must quote the value to cast to string!
  # - name: ENVIROMENT
  #   value: dev
  # - name: ENVIROMENT1
  #   value: true

envFrom:
  []
  # examples:
  # please note that when the value is a number, you must quote the value to cast to string!
  # - configMapRef
  #     name: postgres-configs

# --volumes specifies pod volumes
volumes:
  []
  # - name: sensitive-config
  #   secret:
  #     secretName: app-secrets

# --volumeMounts specifies where Kubernetes will mount Pod volumes
volumeMounts:
  []
  # - name: sensitive-config
  #   mountPath: /sensitive-config/
  #   readOnly: true

# --ResourceQuota provides constraints that limit aggregate resource consumption per namespace
quota:
  # --Specifies whether a resource quota should be created
  enabled: false
  # --resources Specifies the hard resources
  resources:
    hard:
      # --requests.cpu Specifies the total CPU requests allowed for all pods in the namespace
      requests.cpu: "1"
      # --requests.memory Specifies the total memory requests allowed for all pods in the namespace
      requests.memory: 1Gi
      # --limits.cpu Specifies the total CPU limits allowed for all pods in the namespace
      limits.cpu: "2"
      # --limits.memory Specifies the total memory limits allowed for all pods in the namespace
      limits.memory: 2Gi

# --migration Set liquibase migration
migration:
  # -- migration.enable liquibase migration
  enabled: false

# --monitoring Enable Monitoring Features
monitoring:
  rules:
    # -- If enabled, create PrometheusRule resource with app recording rules
    enabled: false
    # -- Include alerting rules
    alerting: true
    # -- Alternative namespace to create recording rules PrometheusRule resource in
    namespace: null
    # -- Additional annotations for the rules PrometheusRule resource
    annotations: {}
    # -- Additional labels for the rules PrometheusRule resource
    labels: {}
    # -- Additional groups to add to the rules file
    additionalGroups: []
    # - name: additional-app-rules
    #   rules:
    #     - record: job:app_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(app_request_duration_seconds_bucket[1m])) by (le, job)
    #     - record: job_route:app_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(app_request_duration_seconds_bucket[1m])) by (le, job, route)
    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)

  # Alerting rules for monitoring app
  alerts:
    # -- If enabled, create PrometheusRule resource with app alerting rules
    enabled: false
    # -- Alternative namespace to create alerting rules PrometheusRule resource in
    namespace: null
    # -- Additional annotations for the alerts PrometheusRule resource
    annotations: {}
    # -- Additional labels for the alerts PrometheusRule resource
    labels: {}

  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: false
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: 60s
    # -- ServiceMonitor path to scrape
    path: /metrics
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: 15s
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    relabelings: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    extraPort:
      # -- If enabled, an additional port will be added to the ServiceMonitor
      enabled: false
      # -- The name of the additional port
      name: tcp-metrics
      # -- The port number for the additional port
      number: 9090
      # -- The target port in the container for the additional port
      targetPort: 9090
      # -- The protocol used for the additional port
      protocol: TCP

# --logging configuration
logging:
  # -- If enabled, create a PodLogs resource for logging
  podLogs:
    enabled: true
    relabelings:
      - sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: __host__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        sourceLabels:
          - __meta_kubernetes_namespace
        targetLabel: namespace
      - action: replace
        sourceLabels:
          - __meta_kubernetes_pod_name
        targetLabel: pod
      - action: replace
        sourceLabels:
          - __meta_kubernetes_container_name
        targetLabel: container
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        sourceLabels:
          - __meta_kubernetes_pod_uid
          - __meta_kubernetes_pod_container_name
        targetLabel: __path__

# ExternalSecrets configuration
externalSecrets:
  # -- If enabled, ExternalSecret resources will be created to sync secrets from external sources (e.g., Vault).
  enabled: false
  # -- The time interval at which secrets are refreshed from the external source (e.g., Vault).
  # Default value is "30s", meaning secrets will be refreshed every 30 seconds.
  refreshInterval: "30s"
  secretStoreRef:
    # -- The kind of SecretStore used to fetch secrets.
    # By default, this is set to "ClusterSecretStore" to allow cluster-wide secret management.
    kind: ClusterSecretStore
    # -- The name of the SecretStore backend. For Vault, this typically refers to the Vault connection (e.g., "vault-backend").
    name: vault-backend
  target:
    # -- The creation policy for the target Kubernetes Secret.
    # "Owner" means ExternalSecret manages the lifecycle of the created secret, deleting it when ExternalSecret is deleted.
    creationPolicy: Owner
  # -- List of secrets to be synced from the external source (e.g., Vault).
  # Add secrets here, where each secretKey in Kubernetes will map to a corresponding key in the external store.
  secrets: []

# --nginx configuration for php-fpm the nginx sidecar
nginx:
  # -- If enabled, create a sidecar container with nginx
  enabled: false
  shared:
    enabled: false
    path: /var/www/html/
  image:
    repository: "nginx"
    tag: "alpine"
    imagePullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 50m
      memory: 50Mi
    requests:
      cpu: 10m
      memory: 10Mi
  livenessProbe:
    enabled: true
    path: /health-check/liveness
    scheme: HTTP
    exec: {}
    # livenessProbe.initialDelaySeconds -- Number of seconds after the container has started before liveness probes are initiated
    initialDelaySeconds: 5
    # livenessProbe.timeoutSeconds -- Number of seconds after which the probe times out
    timeoutSeconds: 3
    # livenessProbe.periodSeconds -- How often (in seconds) to perform the probe
    periodSeconds: 10
    # livenessProbe.failureThreshold -- Minimum consecutive failures for the probe to be considered failed after having succeeded
    failureThreshold: 3
    # livenessProbe.successThreshold -- Minimum consecutive successes for the probe to be considered successful after having failed
    successThreshold: 1

# --extraContainer configuration for an extra container in the pod
extraContainer:
  # -- If enabled, create an extra container in the pod
  enabled: false
  name: extra-container
  image: extra-container-image:latest
  imagePullPolicy: Always
  env:
    - name: ENV_VAR
      value: "value"
  port:
    name: http
    containerPort: 8081
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "128Mi"
      cpu: "200m"
  volumeMounts:
    - name: config-volume
      mountPath: /etc/config

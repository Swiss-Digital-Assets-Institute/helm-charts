---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "helm-ipfs-cluster.name" . }}-cluster
spec:
  serviceName: {{ include "helm-ipfs-cluster.name" . }}-cluster
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "helm-ipfs-cluster.selectorLabels" . | nindent 6 }}
      nodeType: cluster
  template:
    metadata:
      labels:
        {{- include "helm-ipfs-cluster.selectorLabels" . | nindent 8 }}
        nodeType: cluster
      {{- if .Values.metrics.enabled }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/path: "/metrics"
      {{- end }}
    spec:
      {{- with .Values.cluster.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "helm-ipfs-cluster.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.cluster.podSecurityContext | nindent 8 }}
      initContainers:
      - name: init-cluster
        image: "{{ .Values.cluster.image.repository }}:{{ .Values.cluster.image.tag | default .Chart.AppVersion }}"
        command: ["/bin/sh", "-c"]
        args:
          - |
            if [ ! -f /data/ipfs-cluster/service.json ]; then
              echo "Initializing IPFS Cluster..."
              ipfs-cluster-service init
            else
              echo "IPFS Cluster already initialized"
            fi
        env:
          - name: CLUSTER_PEERNAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
{{- if .Values.sharedSecret }}
          - name: CLUSTER_SECRET
            valueFrom:
              secretKeyRef:
                name: {{ include "helm-ipfs-cluster.fullname" . }}
                key: secret
{{- end }}
        volumeMounts:
        - name: data
          mountPath: /data/ipfs-cluster
      {{- if .Values.bootstrap.enabled }}
      - name: configure-bootstrap
        image: curlimages/curl:8.7.1
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -euo pipefail
            POD_INDEX=$(echo "$HOSTNAME" | grep -oE '[0-9]+$')
            
            if [ "$POD_INDEX" != "0" ]; then
              host="{{ include "helm-ipfs-cluster.name" . }}-cluster-0.{{ include "helm-ipfs-cluster.name" . }}-cluster.{{ .Release.Namespace }}.svc.cluster.local"
              echo "Waiting for $host:9094 to be ready..."
              until curl -fsS "http://$host:9094/id" -o /tmp/id.json 2>/dev/null; do 
                sleep 2
              done
              
              addr=$(grep -oE '/(dns4|ip4|dns|dns6|ip6)[^"[:space:]]+/p2p/[A-Za-z0-9]+' /tmp/id.json | head -n1 || true)
              
              if [ -z "$addr" ]; then
                echo "Failed to discover bootstrap address" >&2
                exit 1
              fi
              
              echo "Will bootstrap to: $addr"
              echo "$addr" > /tmp/bootstrap_addr
            else
              echo "This is pod-0, no bootstrap needed"
              touch /tmp/bootstrap_addr
            fi
        volumeMounts:
        - name: bootstrap-info
          mountPath: /tmp
      {{- end }}
      containers:
      - name: cluster
        image: "{{ .Values.cluster.image.repository }}:{{ .Values.cluster.image.tag | default .Chart.AppVersion }}"
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -euo pipefail
            
            # Get pod index
            POD_INDEX=$(echo "$HOSTNAME" | grep -oE '[0-9]+$')
            
            # Set IPFS multiaddress
            export CLUSTER_IPFSHTTP_NODEMULTIADDRESS="/dns4/{{ include "helm-ipfs-cluster.ipfs-name" . }}-${POD_INDEX}.{{ include "helm-ipfs-cluster.ipfs-name" . }}-headless.{{ .Release.Namespace }}.svc.cluster.local/tcp/5001"
            
            BOOT_ARGS=""
            
            {{- if .Values.bootstrap.enabled }}
            # Add external bootstrap peers
            if [ -n "${BOOTSTRAP_PEERS:-}" ]; then
              for a in ${BOOTSTRAP_PEERS}; do 
                BOOT_ARGS="$BOOT_ARGS --bootstrap $a"
              done
            fi
            
            # Add internal bootstrap
            if [ -f /tmp/bootstrap_addr ] && [ -s /tmp/bootstrap_addr ]; then
              BOOTSTRAP_ADDR=$(cat /tmp/bootstrap_addr)
              if [ -n "$BOOTSTRAP_ADDR" ]; then
                BOOT_ARGS="$BOOT_ARGS --bootstrap $BOOTSTRAP_ADDR"
                echo "Bootstrapping with: $BOOT_ARGS"
              fi
            fi
            {{- end }}
            
            exec ipfs-cluster-service daemon $BOOT_ARGS
        env:
          - name: CLUSTER_PEERNAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
{{- if .Values.sharedSecret }}
          - name: CLUSTER_SECRET
            valueFrom:
              secretKeyRef:
                name: {{ include "helm-ipfs-cluster.fullname" . }}
                key: secret
{{- end }}
          {{- if .Values.bootstrap.enabled }}
          - name: BOOTSTRAP_PEERS
            value: "{{ join " " .Values.bootstrap.peers }}"
          {{- end }}
          # Cluster configuration
          - name: CLUSTER_CRDT_TRUSTEDPEERS
            value: '*'
          - name: CLUSTER_RESTAPI_HTTPLISTENMULTIADDRESS
            value: /ip4/0.0.0.0/tcp/9094
          - name: CLUSTER_MONITORPINGINTERVAL
            value: 2s
          {{- if .Values.metrics.enabled }}
          # Prometheus metrics configuration
          - name: CLUSTER_METRICS_ENABLESTATS
            value: "true"
          - name: CLUSTER_METRICS_PROMETHEUSENDPOINT
            value: /ip4/0.0.0.0/tcp/8888
          {{- end }}
        ports:
        - containerPort: 9096
          name: p2p
          protocol: TCP
        - containerPort: 9094
          name: api
          protocol: TCP
        {{- if .Values.metrics.enabled }}
        - containerPort: 8888
          name: metrics
          protocol: TCP
        {{- end }}
        {{- if and .Values.cluster.livenessProbe .Values.cluster.livenessProbe.enabled }}
        livenessProbe:
          httpGet:
            path: /health/graph
            port: 9094
          initialDelaySeconds: {{ .Values.cluster.livenessProbe.initialDelaySeconds | default 30 }}
          periodSeconds: {{ .Values.cluster.livenessProbe.periodSeconds | default 10 }}
          timeoutSeconds: {{ .Values.cluster.livenessProbe.timeoutSeconds | default 5 }}
          failureThreshold: {{ .Values.cluster.livenessProbe.failureThreshold | default 3 }}
        {{- end }}
        {{- if and .Values.cluster.readinessProbe .Values.cluster.readinessProbe.enabled }}
        readinessProbe:
          httpGet:
            path: /health/graph
            port: 9094
          initialDelaySeconds: {{ .Values.cluster.readinessProbe.initialDelaySeconds | default 15 }}
          periodSeconds: {{ .Values.cluster.readinessProbe.periodSeconds | default 10 }}
          timeoutSeconds: {{ .Values.cluster.readinessProbe.timeoutSeconds | default 5 }}
          failureThreshold: {{ .Values.cluster.readinessProbe.failureThreshold | default 3 }}
        {{- end }}
        volumeMounts:
        - name: data
          mountPath: /data/ipfs-cluster
        {{- if .Values.bootstrap.enabled }}
        - name: bootstrap-info
          mountPath: /tmp
        {{- end }}
        resources:
          {{- toYaml .Values.cluster.resources | nindent 12 }}
      volumes:
      {{- if .Values.bootstrap.enabled }}
      - name: bootstrap-info
        emptyDir: {}
      {{- end }}
      {{- with .Values.cluster.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.cluster.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.cluster.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      {{- with .Values.cluster.storage.storageClassName }}
      storageClassName: {{ . }}
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.cluster.storage.volumeSize | default "1Gi" }}